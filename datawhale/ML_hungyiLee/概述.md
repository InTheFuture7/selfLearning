
## 机器学习的概述

本质在于学习到一个函数。

语音识别领域，函数的输入一段语音，函数的输出为语音的文本

图像识别领域，函数的输入为一张图片，函数的输出图片的类别


## 机器学习的分类

根据函数的不同，机器学习进一步分为不同的

分类（classification）：给定一些类别（class）和待定类别的样本，输出样本所属的类别。例如：在 alphaGO 下围棋中，输入当前黑白子的位置，输出下一个落子的位置。

回归（regression）：函数输出一个数值（scale）。例如：根据今天中午的 pm2.5 浓度、气温、$O_3$ 浓度，输出明天中午的 pm2.5 浓度。

结构化学习（structured learning）：产生一个结构化的物体，比如画一张图片、写一篇文档。


## 以 youtube 视频的观看量为例 -- 机器学习的运作过程

机器寻找函数的过程：

### 定义函数

写出一个带有未知参数的函数 $f$，用于预测观看次数。例如：$y = b + wx_1$，其中，$y$ 为要预测的观看次数，$x_1$ 表示前一天的观看次数，$w$ 和 $b$ 都是未知参数。

带有未知参数（parameter）的函数称为模型（model），未知参数 $w$ 称为权重（weight），$b$ 称为偏置（bias）。$x_1$ 视为特征（feature），是已知的数据。

一般需要结合领域知识（domain knowledge）来设计函数。

### 定义损失

损失函数（loss）：关于未知量 $w$、$b$ 的函数 $L(w, b)$，由一组 $w$、$b$ 估测的 $\hat{y}$ 和真实值 $y$ 的差距 $e$ 累计计算得到损失值。

计算预估值 $y_hat$ 和真实值 $y$ 的差距有如下几种方式：
 - 平均绝对误差（Mean Absolute Error， MAE）$e_1 = |y-\hat{y}|$
 - 均方误差（Mean Squared Error, MSE）$e_2 = (y-\hat{y})^2$
 - 交叉熵损失（Cross Entropy）

累计方式：$L = \frac{1}{N} \sum_{n} e_n$。其中，$N$ 表示训练数据的个数，$e_n$ 表示第 n 个样本的损失值

以不同的参数计算损失值，画出的等损失线图称为**误差表面（error surface）**，可以直观看出参数取值与损失值的关系。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202507311112689.png)

上图中，横轴为参数 $w$ 的取值，纵轴为参数 $b$ 的取值，横纵轴交叉点为损失值，将相同损失值对应的 $(w, b)$ 用线连接就得到等损失线。




### 求解最优化问题

找出一组使得损失最小的 $w$ 和 $b$。具体优化（optimization）的步骤如下：

#### 随机初始化

初始化参数分别为：$w^0$、$b^0$

#### 更新

$w = w^0 - \eta \frac{\partial L}{\partial w} |_{w=w^0}$

$b = b^0 - \eta \frac{\partial L}{\partial b} |_{b=b^0}$

> $L$ 关于参数 $w$ 的梯度（$\frac{\partial L}{\partial w}$）指向函数值（损失）增加最快的方向，为了使损失变小，所以增加负号，选择最小化损失的方向

上式中，$\eta$ 为学习率（learning rate），取正值。如果 $\eta$ 越大，那么参数更新量会更大，学习更快，反之则变化小。这一参数需要自己设定，不由机器学习，称为超参数（hyperparameter）。



参数更新的终止：
1. 设定更新次数，超过次数后不再更新
2. 如果偏导数为0，那么参数无法再继续更新




#### 以单参数可视化理解优化过程

下面以一个参数 $w$ 来讨论优化过程

首先，初始化参数 $w=w^0$，然后更新 $w^1 = w^0-\eta \frac{\partial L}{\partial w} |_{w=w^0}$，其中 $\frac{\partial L}{\partial w} |_{w=w^0}$ 在几何上可以理解为损失 $L$ 在 $w^0$ 处的斜率，如果这一数值为负，那么增大 $w$ 就可以实现减小损失，如果这一数值为正，那么减小 $w$ 就能减小损失。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202507311910122.png)

> 上图损失函数有一段为负值。一般来说，损失是估计值和真实值的绝对值差距，不可能为负数，如果人工定义时为绝对值减去某个数值，那么就可能为负数。

> 基于梯度下降的优化方法，可能会遇到局部最小值（local minima）问题，也就是陷入一个局部最优中，无法找到全局最小值（global minima），但是在做梯度下降时，真正面对的问题不是局部最小值。❓



#### 在二维坐标系中的理解

想象一个二维坐标系，横轴是参数 `w`，纵轴是参数 `b`。对于每一对 `(w, b)`，我们都可以计算出一个损失值 `L`。我们可以把损失值 `L` 想象成这个二维平面上的“海拔高度”。这样就形成了一个三维的“山谷”地形，也就是上文中提到的**误差表面（error surface）**。我们的目标是走到“山谷”的最低点。

梯度下降的过程就像这样：

1. **随机选点**：我们随机选择一个出发点 $(w^0, b^0)$。
2. **计算梯度**：在 $(w^0, b^0)$ 这个点，我们计算出梯度 $(∂L/∂w, ∂L/∂b)$。这个梯度向量指向该点最陡峭的**上坡**方向。
3. **反向移动**：我们将当前位置 $(w^0, b^0)$ 沿着梯度的**相反方向**移动一小步（步长由 `η` 控制），到达新的点 $(w^1, b^1)$。
    - $w^1 = w^0 - η * (∂L/∂w)$
    - $b^1 = b^0 - η * (∂L/∂b)$
4. **重复**：在新的点 $(w^1, b^1)$ 重复第 2 和第 3 步，一步步地走向“山谷”的更深处，直到我们到达一个局部或全局的最低点（此时梯度接近于零，参数不再更新）。




### 重新定义函数

在观察实际浏览量数据后，发现浏览量随时间存在周期性规律：周为一个循环周期；每周有两天浏览量较低。

基于数据特点，增加特征数量，优化线性模型为：$y = b + \sum_{j=1}^{7} w_j x_j$




## 模型

如果模型定义为一个或多个输入特征 $x$ 乘上对应权重，再加上一个偏置，这种模型称为线性模型（linear model），根据输入特征的个数，可以分为一元或多元线性模型。

线性模型往往导致预测结果和前一天的特征 $x$ 成一定比例，比如前一天观看量越大，预测的第二天结果也越大。但实际中可能存在周天至周四观看量很高，但是周五、周六观看量较低的周期性变化，所以线性模型无法模拟真实情况，这称为线性模型的偏差（model bias）。

分析观看量数据，考虑观看量是一个复杂的连续曲线，而任何连续曲线可以由分段线性曲线拟合，如下图所示。所以可以将预测观看量问题转化为如何拟合一个分段线性曲线（piecewise linear curve），并由此设计一个更优的模型。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202508111440266.png)

### 分段线性函数拟合

如下图所示，红色的为一个分段线性曲线，蓝色的为常数和一组hard sigmoid。根据每段区间上各曲线的斜率，可以看出蓝色基本可以拟合出红色的分段线性曲线。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202508111444357.png)


综上分析，**任意曲线 ≈ 分段线性曲线 = 常数 + 一组hard sigmoid**。

hard sigmoid 是一个分段函数，数学表达式形如：$y = max(0, min(1, (x + 1)/2))$，难以写出简洁的函数表达式，且不易计算梯度值。所以，通常采用 sigmoid 来代替 hard sigmoid，sigmoid 函数表达式为：

$$
y = c \frac{1}{1+e^{wx+b}} = c \sigma(wx+b)
$$

上式中，$c$ 影响高度，$w$ 影响斜线的斜率，$b$ 影响 $x$ 轴平移位置。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202508111452228.png)

因此，上图中的红色分段线性曲线可以写出数学表达式：$y = b + \sum_{i} c_i \sigma(b_i+w_i x)$

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202508111457815.png)







上面是将模型从线性模型，扩展为一个能拟合任意曲线的模型，增强了模型的灵活性（flexible），下面从输入特征个数出发，进一步增强模型的表示能力。

对于 $sigmoid$ 函数来说，只能接收一个数值作为输入，如果增加多个特征作为输入，一个自然的想法是对多个特征做线性组合。第 $i$ 个sigmoid的输入为：$r_i=b_i+w_{i1}x_1+w_{i2}x_2+...w_{ij}x_j+...=b_i+\sum_j{w_{ij}x_j}$，其中，$x_j$ 是第 $j$ 个特征，表示第 $j$ 天的观看量。

最终模型表达式为：$$y=b+\sum_i{a_i}=b+\sum{\sigma(r_i)}=b+\sum_i{c_i*\sigma(b_i+\sum_j{w_{ij}x_j})}$$

矩阵表示形式（以 $i=3$ 为例）：

$$
y = b + [c_1, c_2, c_3] \cdot sigmoid(
\left[\begin{array}{l}
b_1 \\
b_2 \\
b_3
\end{array}\right]+\left[\begin{array}{lll}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23} \\
w_{31} & w_{32} & w_{33}
\end{array}\right] \times\left[\begin{array}{l}
x_1 \\
x_2 \\
x_3
\end{array}\right]
) \\
y=b+\vec{C}^{\top} \cdot \operatorname{sigmoid}(\vec{b}+\matrix{W}\vec{x})
$$

总结，首先保持特征数量不变，通过引入sigmoid函数，增强模型的非线性表征能力，然后增加特征数量进一步增强模型的灵活性。

单个特征的形式
$$y=b+wx --> b+\sum_i{c_i*\sigma(b_i+w_i x)}$$

多个特征
$$y=b+w_{ij}x_j -> b+\sum_i{c_i*\sigma(b_i+\sum_j{w_{ij}x_j})}$$



### 定义损失

令 $\boldsymbol{\theta}=\{W, b, \vec{c^T}, \vec{b}\}=\boldsymbol{\theta}=\left[\begin{array}{c}
\theta_1 \\
\theta_2 \\
\theta_3 \\
\vdots
\end{array}\right]$，损失函数为 $L(\boldsymbol{\theta})$。目标是找到一组使得损失值最小的一组 $\boldsymbol{\theta}$，称为 $\boldsymbol{\theta}^*$。



### 梯度下降

初始化：$\boldsymbol{\theta}^0$

第一轮参数更新：
$$
\begin{gathered}
\boldsymbol{g_0}=\nabla L\left(\boldsymbol{\theta}_0\right) =
\left[\begin{array}{c}
\left.\frac{\partial L}{\partial \theta_1}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_0} \\
\left.\frac{\partial L}{\partial \theta_2}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_0} \\
\vdots
\end{array}\right]
\end{gathered}
$$

$$
\boldsymbol{\theta_1} = \left[\begin{array}{c}
\theta_1^1 \\
\theta_1^2 \\
\vdots
\end{array}\right] =
\boldsymbol{\theta_0} - \eta \boldsymbol{g_0}=
\left[\begin{array}{c}
\theta_0^1 \\
\theta_0^2 \\
\vdots
\end{array}\right]-\left[\begin{array}{c}
\left.\eta \frac{\partial L}{\partial \theta_1}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_0} \\
\left.\eta \frac{\partial L}{\partial \theta_2}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_0} \\
\vdots
\end{array}\right]
$$

第二轮参数更新：
$$
\begin{gathered}
\boldsymbol{g_1}=\nabla L\left(\boldsymbol{\theta}_1\right) =
\left[\begin{array}{c}
\left.\frac{\partial L}{\partial \theta_1}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_1} \\
\left.\frac{\partial L}{\partial \theta_2}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_1} \\
\vdots
\end{array}\right]
\end{gathered}
$$

$$
\boldsymbol{\theta_2} = \left[\begin{array}{c}
\theta_2^1 \\
\theta_2^2 \\
\vdots
\end{array}\right] =
\boldsymbol{\theta_1} - \eta \boldsymbol{g_1}=
\left[\begin{array}{c}
\theta_1^1 \\
\theta_1^2 \\
\vdots
\end{array}\right]-\left[\begin{array}{c}
\left.\eta \frac{\partial L}{\partial \theta_1}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_1} \\
\left.\eta \frac{\partial L}{\partial \theta_2}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_1} \\
\vdots
\end{array}\right]
$$

不断迭代，直到：设定更新次数，超过次数后不再更新；如果偏导数为0，那么参数无法再继续更新



在进行迭代时，会将数据划分成若干份，每份数据称为一个批量（batch），模型会计算在每个批量上的损失，然后更新（update）一次参数，再计算在下一个批量上的损失，在所有批量上更新一轮称为一个回合（epoch）。



### 模型变形 -- ReLU

表征 hard sigmoid 可以使用 Rectified Linear Unit(ReLU)，ReLU 函数的一般表达式为：$y = max(0, x)$。

一般的，一个 hard sigmoid 函数需要使用两个 ReLU 来表示。从 hard sigmoid 有两个拐点。下面举一个特例来展示两者的关系

下图直观展示，ReLU 和 hard sigmoid 的关系。其中，红色的 hard sigmoid，可以使用绿色的、由两个 ReLU 表示函数来表示。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202509012031352.png)



为什么使用 ReLU 更好？计算效率极高:ReLU 的操作极其简单：`max(0, x)`。这只是一个比较和一个选择操作。为什么不直接使用hard sigmoid，也不过是比较、简单加减乘除



## 引出对神经网络的理解

sigmoid 或 ReLU 等激活函数对向量的非线性处理部分称为神经元（neuron ），很多的神经元构成神经网络（neural network）。

每一排称为一层，称为隐藏层，堆叠多个隐藏层就构成深度学习（deep learning）。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202509012055239.png)

## 模型训练分析



假设训练集为：$\{(x^1, y^1), (x^2, y^2), ...(x^N, y^N)\}$，测试集为：$\{x^{N+1}, x^{N+2}, ...\}$

模型定义为：$y=f_{\boldsymbol{\theta}}(x)$，其中 $\boldsymbol{\theta}$ 表示模型中的所有参数

损失函数定义为：$L(\boldsymbol{\theta})$

最优的参数组合为：$\boldsymbol{\theta}^*=arg(min L)$

最终训练好的模型为：$y=f_{\boldsymbol{\theta}^*}(x)$，将测试集作为输入，即可得到模型对测试集的预测结果

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202509012057576.png)



当训练损失逐渐减小，而测试集的损失值先减小后增大，表明需要增加更多的特征和参数，增加模型复杂度





一般竞赛中会有两个测试集，一个是公开榜排名中的测试集（public testing data），另一个是在竞赛结束后用于测试模型的测试集（private testing data）。如果过度关注公开榜的测试集来修改模型，可能会影响在private testing data中的结果。竞赛结果的评价，会根据模型在公开榜和私榜结果综合评价，但是私榜的权重会更大。

| 数据集划分                                   | 竞赛中的数据集                 |
| -------------------------------------------- | ------------------------------ |
| 训练集training set                           | 训练集training set             |
| 验证集（从训练集中划分一部分比例作为验证集） | 公开榜测试集public testing set |
| 测试集testing set                            | 私榜测试集private testing set  |



k折交叉验证（k-fold cross validation）

步骤：假设当前只有训练集和验证集，将训练集划分成 $k$ 份，取第 $1$ 份作为验证集，取后面 $k-1$ 份作为训练集，模型在 $k-1$ 份数据上训练特定的轮数（epoch），然后在验证集上评估，得到验证集上的损失值。然后取第 $2$ 份作为验证集，取剩下 $k-1$ 作为训练集，得到在这一组数据下的损失值。计算验证集的平均指标值作为该组参数下模型的指标。

作用：避免因为数据集过小，导致模型评估缺乏可靠性

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202509012057174.png)



训练集、验证集、测试集的作用

训练集：训练模型，调整模型的参数

验证集：根据不同超参数组合在验证集上的效果，选择最优超参数并选择模型

测试集：对模型做最终的性能评估



### 局部最小值与鞍点

现象：随着不断更新参数，训练的损失不再下降，但是损失值仍不满意。

原因：优化到某个阶段，关于参数的微分/梯度为零，导致参数更新停止。

梯度为零的点（临界点， critical point）：局部最小值（local minima）和鞍点（saddle point）。其中，鞍点是局部极大值（local maximum）点。

判断临界点的类型


### 批量与动量





❓没理解视频内容

随着update的增加，损失函数的变化特点：什么逐渐减小，但是不够小；什么保持平稳





梯度下降和back propagation的关系是什么？

