
## 机器学习的概述

本质在于学习到一个函数。

语音识别领域，函数的输入一段语音，函数的输出为语音的文本

图像识别领域，函数的输入为一张图片，函数的输出图片的类别


## 机器学习的分类

根据函数的不同，机器学习进一步分为不同的

分类（classification）：给定一些类别（class）和待定类别的样本，输出样本所属的类别。例如：在 alphaGO 下围棋中，输入当前黑白子的位置，输出下一个落子的位置。

回归（regression）：函数输出一个数值（scale）。例如：根据今天中午的 pm2.5 浓度、气温、$O_3$ 浓度，输出明天中午的 pm2.5 浓度。

结构化学习（structured learning）：产生一个结构化的物体，比如画一张图片、写一篇文档。


## 以 youtube 视频的观看量为例 -- 机器学习的运作过程

机器寻找函数的过程：

### 定义函数

写出一个带有未知参数的函数 $f$，用于预测观看次数。例如：$y = b + wx_1$，其中，$y$ 为要预测的观看次数，$x_1$ 表示前一天的观看次数，$w$ 和 $b$ 都是未知参数。

带有未知参数（parameter）的函数称为模型（model），未知参数 $w$ 称为权重（weight），$b$ 称为偏置（bias）。$x_1$ 视为特征（feature），是已知的数据。

一般需要结合领域知识（domain knowledge）来设计函数。

### 定义损失

损失函数（loss）：关于未知量 $w$、$b$ 的函数 $L(w, b)$，由一组 $w$、$b$ 估测的 $\hat{y}$ 和真实值 $y$ 的差距 $e$ 累计计算得到损失值。

计算预估值 $y_hat$ 和真实值 $y$ 的差距有如下几种方式：
 - 平均绝对误差（Mean Absolute Error， MAE）$e_1 = |y-\hat{y}|$
 - 均方误差（Mean Squared Error, MSE）$e_2 = (y-\hat{y})^2$
 - 交叉熵损失（Cross Entropy）

累计方式：

$$L = \frac{1}{N} \sum_{n} e_n$$

上式中，$N$ 表示训练数据的个数，$e_n$ 表示第 n 个样本的损失值

以不同的参数计算损失值，画出的等损失线图称为误差表面（error surface），可以直观看出参数取值与损失值的关系。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202507311112689.png)

上图中，横轴为参数 $w$ 的取值，纵轴为参数 $b$ 的取值，横纵轴交叉点为损失值，并将相同损失值对应的 $(w, b)$ 用线连接。


### 求解最优化问题

找出一组使得损失最小的 $w$ 和 $b$

具体优化（optimization）的步骤

#### 随机初始化

初始化参数分别为：$w^0$、$b^0$

#### 更新

$w = w^0 - \eta \frac{\partial L}{\partial w} |_{w=w^0}$

$b = b^0 - \eta \frac{\partial L}{\partial b} |_{b=b^0}$

上式中，$\eta$ 为学习率（learning rate），如果 $\eta$ 越大，那么参数更新量会更大，学习更快，反之则变化小。这一参数需要自己设定，不由机器学习，称为超参数（hyperparameter）。

参数更新的终止：
1. 设定更新次数，超过次数后不再更新
2. 如果偏导数为0，那么参数无法再继续更新


#### 以单参数可视化理解优化过程

下面以一个参数 $w$ 来讨论优化过程

首先，初始化参数 $w=w^0$，然后更新 $w^1 = w^0-\eta \frac{\partial L}{\partial w} |_{w=w^0}$，其中 $\frac{\partial L}{\partial w} |_{w=w^0}$ 在几何上可以理解为损失 $L$ 在 $w^0$ 处的斜率，如果这一数值为负，那么增大 $w$ 就可以实现减小损失，如果这一数值为正，那么减小 $w$ 就能减小损失。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202507311910122.png)

> 上图损失函数有一段为负值。一般来说，损失是估计值和真实值的绝对值差距，不可能为负数，如果人工定义时为绝对值减去某个数值，那么就可能为负数。

> 基于梯度下降的优化方法，可能会遇到局部最小值（local minima）问题，也就是陷入一个局部最优中，无法找到全局最小值（global minima），但是在做梯度下降时，真正面对的问题不是局部最小值。❓


### 重新定义函数

在观察实际浏览量数据后，发现浏览量随时间存在周期性规律：周为一个循环周期；每周有两天浏览量较低。

基于数据特点，重新修改模型：

$y = b + \sum_{j=1}^{7} w_j x_j$

> 上面仍然还是线性模型❓


## 模型

如果模型定义为输入的特征 $x$ 乘上一个权重，再加上一个偏置，这种模型称为线性模型（linear model）

线性模型往往导致预测结果和前一天的特征 $x$ 成一定比例，比如前一天观看量越大，预测的第二天结果也越大。但实际中可能存在周天至周四观看量很高，但是周五、周六观看量较低的周期性变化，所以线性模型无法模拟真实情况，这称为线性模型的偏差（model bias）。

考虑观看量是一个复杂的连续曲线，而任何连续曲线可以由分段线性曲线拟合，如下图所示。所以，预测观看量问题可以转化为如何拟合一个分段线性曲线（piecewise linear curve）。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202508111440266.png)

如下图所示，红色的为一个分段线性曲线，蓝色的为常数和一组hard sigmoid。根据每段区间上各曲线的斜率，可以看出蓝色基本可以拟合出红色的分段线性曲线。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202508111444357.png)

hard sigmoid 是一个分段函数，不容易写出简洁的函数表达式，所以采用 sigmoid 来代替 hard sigmoid，sigmoid 函数表达式为：

$$
y = c \frac{1}{1+e^{wx+b}} = c \sigma(wx+b)
$$

上式中，c影响高度，w影响斜线的斜率，b影响x轴平移位置

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202508111452228.png)

因此，上图中的红色分段线性曲线可以写出数学表达式，几何表示如下图所示。

$$y = b + \sum_{i} c_i \sigma(b_i+w_i x)$$ 

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202508111457815.png)

上面是将模型从线性模型，扩展为一个能拟合任意曲线的模型，增强了模型的灵活性（flexible），下面从输入特征个数出发，进一步增强模型的表示能力。

设 $x_j$ 表示前第 j 天的观看量，$j=1,2,3$

$$y=b+\sum_i{c_i*\sigma(b_i+\sum_j{w_{ij}x_j})}$$

> ❓TODO:为什么多特征是这样写表达式？

令 $r_i = b_i+\sum_j{w_{ij}x_j}$

$a_i = \sigma(r_i)$

$y = b + \sum_i{a_i}$

矩阵表示形式：



---

总结，首先保持特征数量不变，通过引入sigmoid函数，增强模型的非线性表征能力，然后增加特征进一步增强模型的灵活性。


单个特征的形式
$$y=b+wx -> b+\sum_i{c_i*\sigma(b_i+w_i x)}$$

多个特征
$$y=b+w_{ij}x_j -> b+\sum_i{c_i*\sigma(b_i+\sum_j{w_{ij}x_j})}$$



图形或几何上如何理解？为什么从单个特征到多个特征的表达式是：$w_ix$ -> $w_{ij} x_j$



定义损失



模型变形

---


表征 hard sigmoid 可以使用 Rectified Linear Unit(ReLU)，
为什么两个 ReLU 合并才能表示一个 hard sigmoid？
为什么使用 ReLU 更好？


❓不是很理解这一章的思路，如何用通俗易懂的语言描述清楚这一章的内容？


---

❓连续使用100个ReLU作为模型是什么意思？



观察数据后发现，线性模型不易模拟存在周期性的 youtube 订阅数，