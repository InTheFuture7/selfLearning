
## 机器学习的概述

本质在于学习到一个函数。

语音识别领域，函数的输入一段语音，函数的输出为语音的文本

图像识别领域，函数的输入为一张图片，函数的输出图片的类别


## 机器学习的分类

根据函数的不同，机器学习进一步分为不同的

分类（classification）：给定一些类别（class）和待定类别的样本，输出样本所属的类别。例如：在 alphaGO 下围棋中，输入当前黑白子的位置，输出下一个落子的位置。

回归（regression）：函数输出一个数值（scale）。例如：根据今天中午的 pm2.5 浓度、气温、$O_3$ 浓度，输出明天中午的 pm2.5 浓度。

结构化学习（structured learning）：产生一个结构化的物体，比如画一张图片、写一篇文档。


## 以 youtube 视频的观看量为例 -- 机器学习的运作过程

机器寻找函数的过程：

### 定义函数

写出一个带有未知参数的函数 $f$，用于预测观看次数。例如：$y = b + wx_1$，其中，$y$ 为要预测的观看次数，$x_1$ 表示前一天的观看次数，$w$ 和 $b$ 都是未知参数。

带有未知参数（parameter）的函数称为模型（model），未知参数 $w$ 称为权重（weight），$b$ 称为偏置（bias）。$x_1$ 视为特征（feature），是已知的数据。

一般需要结合领域知识（domain knowledge）来设计函数。

### 定义损失

损失函数（loss）：关于未知量 $w$、$b$ 的函数 $L(w, b)$，由一组 $w$、$b$ 估测的 $\hat{y}$ 和真实值 $y$ 的差距 $e$ 累计计算得到损失值。

计算预估值 $y_hat$ 和真实值 $y$ 的差距有如下几种方式：
 - 平均绝对误差（Mean Absolute Error， MAE）$e_1 = |y-\hat{y}|$
 - 均方误差（Mean Squared Error, MSE）$e_2 = (y-\hat{y})^2$
 - 交叉熵损失（Cross Entropy）

累计方式：

$$L = \frac{1}{N} \sum_{n} e_n$$

上式中，$N$ 表示训练数据的个数，$e_n$ 表示第 n 个样本的损失值

以不同的参数计算损失值，画出的等损失线图称为误差表面（error surface），可以直观看出参数取值与损失值的关系。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202507311112689.png)

上图中，横轴为参数 $w$ 的取值，纵轴为参数 $b$ 的取值，横纵轴交叉点为损失值，并将相同损失值对应的 $(w, b)$ 用线连接。


### 求解最优化问题

找出一组使得损失最小的 $w$ 和 $b$

具体优化（optimization）的步骤

#### 随机初始化

初始化参数分别为：$w^0$、$b^0$

#### 更新

$w = w^0 - \eta \frac{\partial L}{\partial w} |_{w=w^0}$

$b = b^0 - \eta \frac{\partial L}{\partial b} |_{b=b^0}$

> ❓为什么是减号？$\eta$ 为正数吗？

上式中，$\eta$ 为学习率（learning rate），如果 $\eta$ 越大，那么参数更新量会更大，学习更快，反之则变化小。这一参数需要自己设定，不由机器学习，称为超参数（hyperparameter）。

参数更新的终止：
1. 设定更新次数，超过次数后不再更新
2. 如果偏导数为0，那么参数无法再继续更新


#### 以单参数可视化理解优化过程

下面以一个参数 $w$ 来讨论优化过程

首先，初始化参数 $w=w^0$，然后更新 $w^1 = w^0-\eta \frac{\partial L}{\partial w} |_{w=w^0}$，其中 $\frac{\partial L}{\partial w} |_{w=w^0}$ 在几何上可以理解为损失 $L$ 在 $w^0$ 处的斜率，如果这一数值为负，那么增大 $w$ 就可以实现减小损失，如果这一数值为正，那么减小 $w$ 就能减小损失。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202507311910122.png)

> 上图损失函数有一段为负值。一般来说，损失是估计值和真实值的绝对值差距，不可能为负数，如果人工定义时为绝对值减去某个数值，那么就可能为负数。

> 基于梯度下降的优化方法，可能会遇到局部最小值（local minima）问题，也就是陷入一个局部最优中，无法找到全局最小值（global minima），但是在做梯度下降时，真正面对的问题不是局部最小值。❓


### 重新定义函数

在观察实际浏览量数据后，发现浏览量随时间存在周期性规律：周为一个循环周期；每周有两天浏览量较低。

基于数据特点，重新修改模型：

$y = b + \sum_{j=1}^{7} w_j x_j$

> 上面仍然还是线性模型❓


## 模型

如果模型定义为输入的特征 $x$ 乘上一个权重，再加上一个偏置，这种模型称为线性模型（linear model）

线性模型往往导致预测结果和前一天的特征 $x$ 成一定比例，比如前一天观看量越大，预测的第二天结果也越大。但实际中可能存在周天至周四观看量很高，但是周五、周六观看量较低的周期性变化，所以线性模型无法模拟真实情况，这称为线性模型的偏差（model bias）。

考虑观看量是一个复杂的连续曲线，而任何连续曲线可以由分段线性曲线拟合，如下图所示。所以，预测观看量问题可以转化为如何拟合一个分段线性曲线（piecewise linear curve）。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202508111440266.png)

如下图所示，红色的为一个分段线性曲线，蓝色的为常数和一组hard sigmoid。根据每段区间上各曲线的斜率，可以看出蓝色基本可以拟合出红色的分段线性曲线。

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202508111444357.png)


综上分析，**任意曲线≈分段线性曲线=常数+一组hard sigmoid**。

hard sigmoid 是一个分段函数，不容易写出简洁的函数表达式，举例某一函数表达式如下：

$$y = max(0, min(1, (x + 1)/2))$$

所以采用 sigmoid 来代替 hard sigmoid，sigmoid 函数表达式为：

$$
y = c \frac{1}{1+e^{wx+b}} = c \sigma(wx+b)
$$

上式中，c 影响高度，w 影响斜线的斜率，b 影响 x 轴平移位置

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202508111452228.png)

因此，上图中的红色分段线性曲线可以写出数学表达式，几何表示如下图所示。

$$y = b + \sum_{i} c_i \sigma(b_i+w_i x)$$ 

![](https://raw.githubusercontent.com/InTheFuture7/attachment/main/202508111457815.png)

上面是将模型从线性模型，扩展为一个能拟合任意曲线的模型，增强了模型的灵活性（flexible），下面从输入特征个数出发，进一步增强模型的表示能力。

设 $x_j$ 表示前第 j 天的观看量，$j=1,2,3$

$$y=b+\sum_i{c_i*\sigma(b_i+\sum_j{w_{ij}x_j})}$$

> ❓TODO:为什么多特征是这样写表达式？

令 $r_i = b_i+\sum_j{w_{ij}x_j}$

$a_i = \sigma(r_i)$

$y = b + \sum_i{a_i}$



矩阵表示形式：

$$
y = b + [c_1, c_2, c_3] \cdot sigmoid(
\left[\begin{array}{l}
b_1 \\
b_2 \\
b_3
\end{array}\right]+\left[\begin{array}{lll}
W_{11} & W_2 & W_{13} \\
W_{21} & W_{22} & W_{23} \\
W_{31} & W_{32} & W_{33}
\end{array}\right] \times\left[\begin{array}{l}
x_1 \\
x_2 \\
x_3
\end{array}\right]
) \\
y=b+\vec{C}^{\top} \cdot \operatorname{sigmoid}(\vec{b}+\matrix{W}\vec{x})
$$


---



总结，首先保持特征数量不变，通过引入sigmoid函数，增强模型的非线性表征能力，然后增加特征数量进一步增强模型的灵活性。


单个特征的形式
$$y=b+wx -> b+\sum_i{c_i*\sigma(b_i+w_i x)}$$

多个特征
$$y=b+w_{ij}x_j -> b+\sum_i{c_i*\sigma(b_i+\sum_j{w_{ij}x_j})}$$



图形或几何上如何理解？为什么从单个特征到多个特征的表达式是：$w_ix$ -> $w_{ij} x_j$



### 定义损失



### 梯度下降

初始化：$\theta^0$

第一轮参数更新：$g=$

第二轮参数更新：



### 模型变形 -- ReLU

表征 hard sigmoid 可以使用 Rectified Linear Unit(ReLU)，ReLU 函数的一般表达式为：

$$y = max(0, x)$$

可以采用 ReLU 函数来表示 hard sigmoid 函数，一般来说 hard sigmoid 有两个拐点，所以每个 hard sigmoid 需要使用两个 ReLU 来表示。下面举一个特例来展示两者的关系

画图展示

hardsigmoid(x) = max(0, min(1, (x + 1)/2))

hardsigmoid(x) = ReLU(x + 1) - ReLU(x - 1)

ReLU(x)=max(0, x)



为什么使用 ReLU 更好？计算效率极高:ReLU 的操作极其简单：`max(0, x)`。这只是一个比较和一个选择操作。为什么不直接使用hard sigmoid，也不过是比较、简单加减乘除




❓不是很理解这一章的思路，如何用通俗易懂的语言描述清楚这一章的内容？



❓连续使用100个ReLU作为模型是什么意思？



观察数据后发现，线性模型不易模拟存在周期性的 youtube 订阅数，



---





## 引出对神经网络的理解

神经元 neuron -> 神经网络 neural network

layer -> deep learning



为什么是deep learning而不是wide or fat learning？





如何根据训练结果来选择模型？

假设训练集为：$\{(x^1, y^1), (x^2, y^2), ...(x^N, y^N)\}$，测试集为：$\{x^{N+1}, x^{N+2}, ...\}$

模型定义为：$y=f_{\theta}(x)$，其中$\theta$表示模型中的所有参数

损失函数定义为：$L(\theta)$

最优的参数组合为：$\theta^*=arg(min L)$

最终训练好的模型为：$y=f_{\theta^*}(x)$，将测试集作为输入，即可得到模型对测试集的预测结果



过拟合（overfit）

为了解决过拟合，而给模型增加限制，可能会带来模型偏差。

当训练损失逐渐减小，而测试集的损失值先减小后增大，表明需要增加更多的特征和参数

模型偏差和模型复杂性的trade off











一帮竞赛中会有两个测试集，一个是公开榜排名中的测试集（public testing data），另一个是在竞赛结束后用于测试模型的测试集（private testing data）。如果过度关注公开榜的测试集来修改模型，可能会影响在private testing data中的结果。竞赛结果的评价，会根据模型在公开榜和私榜结果综合评价，但是私榜的权重会更大。

| 数据集划分                                   | 竞赛中的数据集                 |
| -------------------------------------------- | ------------------------------ |
| 训练集training set                           | 训练集training set             |
| 验证集（从训练集中划分一部分比例作为验证集） | 公开榜测试集public testing set |
| 测试集testing set                            | 私榜测试集private testing set  |

k折交叉验证（k-fold cross validation）

假设当前只有训练集和验证集，将训练集划分成 $k$ 份，取第 $1$ 份作为验证集，取后面 $k-1$ 份作为训练集，模型在 $k-1$ 份数据上训练，然后在验证集上评估，得到验证集上的损失值。然后取第 $2$ 份作为验证集，取剩下 $k-1$ 作为训练集，得到在这一组数据下的损失值。计算平均就得到当前训练集上的损失值，更新模型参数？



❓有个问题：训练集、验证集、测试集，这三个数据的作用是什么？没有理解清楚



## 局部最小值与鞍点



局部最小值（local minima），

鞍点（saddle point），梯度为0，可以走出来



❓没理解视频内容

随着update的增加，损失函数的变化特点：什么逐渐减小，但是不够小；什么保持平稳





梯度下降和back propagation的关系是什么？

