{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试：\n",
    "\n",
    "1、使用 litellm 框架调用大模型\n",
    "\n",
    "2、使用 openai-agent 框架搭建 agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于 litellm 调用大模型\n",
    "\n",
    "使用 litellm 库调用模型，支持的[大模型提供商](https://docs.litellm.ai/docs/providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from litellm import completion\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 基于 litellm，测试三类大模型：deepseek、glm4、mastral\n",
    "\n",
    "# api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "# base_url = \"https://api.deepseek.com/v1\"\n",
    "# chat_model = \"deepseek/deepseek-chat\"\n",
    "\n",
    "chat_model = \"openai/glm-4-flash-250414\"  # openai-compatible\n",
    "base_url=\"https://open.bigmodel.cn/api/paas/v4\"\n",
    "api_key=os.getenv('zhipu_key')\n",
    "\n",
    "# chat_model = \"mistral/mistral-small-latest\"\n",
    "# base_url=\"https://api.mistral.ai/v1\"\n",
    "# api_key=os.getenv('mistral_key')\n",
    "\n",
    "response = completion(\n",
    "    model=chat_model,\n",
    "    messages=[{\"content\": \"你有什么技能？\", \"role\": \"user\"}],\n",
    "    api_base=base_url,\n",
    "    api_key=api_key\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于 openai-agent 搭建 agent 框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled, set_default_openai_api, set_default_openai_client\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 如果切换模型，更改下面的参数即可\n",
    "chat_model = \"deepseek-chat\"\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"https://api.deepseek.com/v1\",\n",
    "    api_key=os.getenv('DEEPSEEK_API_KEY'),\n",
    ")\n",
    "\n",
    "# chat_model = \"glm-4-flash-250414\"\n",
    "# client = AsyncOpenAI(\n",
    "#     base_url=\"https://open.bigmodel.cn/api/paas/v4/\",\n",
    "#     api_key=os.getenv('zhipu_key'),\n",
    "# )\n",
    "\n",
    "# chat_model = \"mistral-small-latest\"\n",
    "# # 支持并发处理\n",
    "# client = AsyncOpenAI(\n",
    "#     base_url=\"https://api.mistral.ai/v1\",\n",
    "#     api_key=os.getenv('mistral_key'),\n",
    "# )\n",
    "\n",
    "# agent 框架默认使用刚配置的 client\n",
    "# 禁止 client 的追踪功能\n",
    "set_default_openai_client(client=client, use_for_tracing=False)\n",
    "# 使用 chat_completion 接口\n",
    "set_default_openai_api(\"chat_completions\")\n",
    "# 全局禁用追踪\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "agent = Agent(name=\"Assistant\", model=chat_model, instructions=\"You are a helpful assistant\")\n",
    "# run_sync() 同步运行，阻塞当前线程直到 agent 执行完成\n",
    "# jupyter notebook 为异步环境（同时运行多个 notebook），所以运行下面代码会报错\n",
    "# result = Runner.run_sync(agent, \"给我讲个程序员相亲的笑话\")\n",
    "\n",
    "# Runner.run 是异步方法，返回一个协程对象，await 会等待协程执行完成并返回结果\n",
    "result = await Runner.run(agent, \"给我讲个程序员相亲的笑话\")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面代码应该在 py 文件中运行\n",
    "from agents import Agent, ModelSettings, enable_verbose_stdout_logging, Runner\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 从环境变量中读取api_key\n",
    "chat_model = \"openai/glm-4-flash-250414\"  # openai-compatible\n",
    "base_url=\"https://open.bigmodel.cn/api/paas/v4\"\n",
    "api_key=os.getenv('zhipu_key')\n",
    "\n",
    "# 调用函数，启用详细日志输出。默认输出警告和错误信息。可以使用 logging 库记录更加详细的日志信息\n",
    "# enable_verbose_stdout_logging()\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)\n",
    "\n",
    "agent = Agent(name=\"Assistant\", \n",
    "              model=llm, \n",
    "              instructions=\"You are a helpful assistant\",\n",
    "              model_settings=ModelSettings(temperature=0.1)  # 配置温度等参数\n",
    "              )\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(agent, \"给我讲个程序员相亲的笑话\")\n",
    "    print(result.final_output)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么存在这么多无关输出？\n",
    "\n",
    "```\n",
    "Tracing is disabled. Not creating trace Agent workflow\n",
    "Setting current trace: no-op\n",
    "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x748cbe9c1760>\n",
    "Running agent Assistant (turn 1)\n",
    "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x748cb62029e0>\n",
    "Calling Litellm model: openai/glm-4-flash-250414\n",
    "[\n",
    "  {\n",
    "    \"content\": \"You are a helpful assistant\",\n",
    "    \"role\": \"system\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"给我讲个程序员相亲的笑话\"\n",
    "  }\n",
    "]\n",
    "Tools:\n",
    "[]\n",
    "Stream: False\n",
    "Tool choice: NOT_GIVEN\n",
    "Response format: NOT_GIVEN\n",
    "\n",
    "LLM resp:\n",
    "{\n",
    "  \"content\":\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异步运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智谱、deepseek、和mistral都不支持异步运行，需要用Runner.run_sync 去运行\n",
    "\n",
    "可以借助 openai 官方提供的 LitellmModel 配合 mistral，支持异步运行\n",
    "\n",
    "❓不理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的，这里有一个关于程序员相亲的笑话，希望能让你会心一笑：\n",
      "\n",
      "---\n",
      "\n",
      "有一个程序员去相亲，见面后女生问他：“你平时都喜欢做什么呀？”\n",
      "\n",
      "程序员想了想，回答：“我喜欢写代码，解决问题，还有……调试。”\n",
      "\n",
      "女生有点困惑：“调试？什么意思？”\n",
      "\n",
      "程序员解释道：“就是当代码出现问题时，我会花很多时间找出错误，然后修复它。”\n",
      "\n",
      "女生点点头：“听起来很有趣。那你平时都写什么代码呢？”\n",
      "\n",
      "程序员自豪地说：“我最近在写一个相亲APP，专门帮助像我这样的程序员找到合适的伴侣。”\n",
      "\n",
      "女生惊讶地问：“真的吗？那你的APP有什么特别之处？”\n",
      "\n",
      "程序员得意地回答：“它会根据用户的代码风格、调试习惯和对技术的热情来匹配合适的伴侣。”\n",
      "\n",
      "女生笑了笑：“那你的APP有没有帮你找到合适的伴侣呢？”\n",
      "\n",
      "程序员尴尬地挠了挠头：“还在调试中……不过，我今天来相亲，就是为了收集更多的用户反馈。”\n",
      "\n",
      "---\n",
      "\n",
      "希望这个笑话能让你开心一笑！\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "# 从环境变量中读取api_key\n",
    "api_key = os.getenv('mistral_key')\n",
    "base_url = 'https://api.mistral.ai/v1'\n",
    "chat_model = \"mistral/mistral-small-latest\"\n",
    "\n",
    "set_tracing_disabled(disabled=True)\n",
    "llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)\n",
    "agent = Agent(name=\"Assistant\", model=llm, instructions=\"You are a helpful assistant\")\n",
    "\n",
    "\n",
    "# 下面的代码可以运行，但是存在大量无关输出！\n",
    "async def main():\n",
    "    result = await Runner.run(agent, \"给我讲个程序员相亲的笑话\")\n",
    "    print(result.final_output)\n",
    "\n",
    "# 在 Notebook 中直接 await\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
