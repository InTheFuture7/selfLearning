{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务：主 agent 接受来自用户的任务，首先确定问题是否能够回答（护栏边界内），如果在护栏内，那么调用对应的 agent 回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from agents import set_tracing_disabled\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "# 从环境变量中读取api_key\n",
    "api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "base_url = \"https://api.deepseek.com/v1\"\n",
    "chat_model = \"deepseek/deepseek-chat\"\n",
    "\n",
    "# chat_model = \"mistral-small-latest\"\n",
    "# base_url=\"https://api.mistral.ai/v1\"\n",
    "# api_key=os.getenv('mistral_key')\n",
    "\n",
    "set_tracing_disabled(disabled=True)\n",
    "llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent\n",
    "\n",
    "# 历史\n",
    "history_tutor_agent = Agent(\n",
    "    name=\"History Tutor\",\n",
    "    handoff_description=\"Specialist agent for historical questions\",\n",
    "    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# 数学\n",
    "math_tutor_agent = Agent(\n",
    "    name=\"Math Tutor\",\n",
    "    handoff_description=\"Specialist agent for math questions\",\n",
    "    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n",
    "    model=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import GuardrailFunctionOutput, InputGuardrail, Agent, Runner\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class HomeworkOutput(BaseModel):\n",
    "    \"\"\"定义输出数据模型\"\"\"\n",
    "    is_homework: bool\n",
    "    reasoning: str\n",
    "\n",
    "# 创建护栏代理\n",
    "guardrail_agent = Agent(\n",
    "    name=\"Guardrail check\",\n",
    "    instructions=\"\"\"Check if the user is asking about homework.\n",
    "    \n",
    "    Return your response in the following JSON format:\n",
    "    {\n",
    "        \"is_homework\": true/false,\n",
    "        \"reasoning\": \"Your reasoning here\"\n",
    "    }\n",
    "    \"\"\",\n",
    "    output_type=HomeworkOutput,  # 约定输出格式\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# 异步函数\n",
    "async def homework_guardrail(ctx, agent, input_data):\n",
    "    # 传入护栏代理、输入数据、上下文\n",
    "    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)\n",
    "    final_output = result.final_output_as(HomeworkOutput)\n",
    "    # 将结果转换为 HomeworkOutput 格式\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=final_output,  # 包含结果的详细信息\n",
    "        tripwire_triggered=not final_output.is_homework,  # 出发条件，如果 is_homework=False，则出发护栏\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于用户问题，选择合适的 agent\n",
    "# triage_agent = Agent(\n",
    "#     name=\"Triage Agent\",\n",
    "#     instructions=\"You determine which agent to use based on the user's homework question\",\n",
    "#     handoffs=[history_tutor_agent, math_tutor_agent],\n",
    "#     model=llm,\n",
    "# )\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"You determine which agent to use based on the user's homework question\",\n",
    "    # 允许代理将特定任务委派给其他代理\n",
    "    handoffs=[history_tutor_agent, math_tutor_agent],\n",
    "    # 可以对输入到代理的内容进行验证\n",
    "    input_guardrails=[\n",
    "        InputGuardrail(guardrail_function=homework_guardrail),\n",
    "    ],\n",
    "    model=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: DeepseekException - Failed to deserialize the JSON body into the target type: response_format: This response_format type is unavailable now at line 1 column 813",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:111\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler._make_common_async_call\u001b[0;34m(self, async_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m async_httpx_client\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    112\u001b[0m         url\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m    113\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    114\u001b[0m         data\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    115\u001b[0m             signed_json_body\n\u001b[1;32m    116\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m signed_json_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    117\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m json\u001b[38;5;241m.\u001b[39mdumps(data)\n\u001b[1;32m    118\u001b[0m         ),\n\u001b[1;32m    119\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    120\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    121\u001b[0m         logging_obj\u001b[38;5;241m=\u001b[39mlogging_obj,\n\u001b[1;32m    122\u001b[0m     )\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/litellm_core_utils/logging_utils.py:135\u001b[0m, in \u001b[0;36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py:324\u001b[0m, in \u001b[0;36mAsyncHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py:280\u001b[0m, in \u001b[0;36mAsyncHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[0m\n\u001b[1;32m    279\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msend(req, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[0;32m--> 280\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '422 Unprocessable Entity' for url 'https://api.deepseek.com/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/main.py:541\u001b[0m, in \u001b[0;36macompletion\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutine(init_response):\n\u001b[0;32m--> 541\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:238\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler.async_completion\u001b[0;34m(self, custom_llm_provider, provider_config, api_base, headers, data, timeout, model, model_response, logging_obj, messages, optional_params, litellm_params, encoding, api_key, client, json_mode, signed_json_body)\u001b[0m\n\u001b[1;32m    236\u001b[0m     async_httpx_client \u001b[38;5;241m=\u001b[39m client\n\u001b[0;32m--> 238\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_common_async_call(\n\u001b[1;32m    239\u001b[0m     async_httpx_client\u001b[38;5;241m=\u001b[39masync_httpx_client,\n\u001b[1;32m    240\u001b[0m     provider_config\u001b[38;5;241m=\u001b[39mprovider_config,\n\u001b[1;32m    241\u001b[0m     api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    244\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    245\u001b[0m     litellm_params\u001b[38;5;241m=\u001b[39mlitellm_params,\n\u001b[1;32m    246\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    247\u001b[0m     logging_obj\u001b[38;5;241m=\u001b[39mlogging_obj,\n\u001b[1;32m    248\u001b[0m     signed_json_body\u001b[38;5;241m=\u001b[39msigned_json_body,\n\u001b[1;32m    249\u001b[0m )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m provider_config\u001b[38;5;241m.\u001b[39mtransform_response(\n\u001b[1;32m    251\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    252\u001b[0m     raw_response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m     json_mode\u001b[38;5;241m=\u001b[39mjson_mode,\n\u001b[1;32m    262\u001b[0m )\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:136\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler._make_common_async_call\u001b[0;34m(self, async_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:2405\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler._handle_error\u001b[0;34m(self, e, provider_config)\u001b[0m\n\u001b[1;32m   2399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BaseLLMException(\n\u001b[1;32m   2400\u001b[0m         status_code\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m   2401\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_text,\n\u001b[1;32m   2402\u001b[0m         headers\u001b[38;5;241m=\u001b[39merror_headers,\n\u001b[1;32m   2403\u001b[0m     )\n\u001b[0;32m-> 2405\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m provider_config\u001b[38;5;241m.\u001b[39mget_error_class(\n\u001b[1;32m   2406\u001b[0m     error_message\u001b[38;5;241m=\u001b[39merror_text,\n\u001b[1;32m   2407\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m   2408\u001b[0m     headers\u001b[38;5;241m=\u001b[39merror_headers,\n\u001b[1;32m   2409\u001b[0m )\n",
      "\u001b[0;31mOpenAIError\u001b[0m: Failed to deserialize the JSON body into the target type: response_format: This response_format type is unavailable now at line 1 column 813",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 测试数学问题（应该通过护栏）\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(triage_agent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m欧拉公式的应用？\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m结果:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mfinal_output)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InputGuardrailTripwireTriggered \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:200\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03moutput is generated. The loop runs like so:\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m1. The agent is invoked with the given input.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    agent. Agents may perform handoffs, so we don't know the specific type of the output.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m runner \u001b[38;5;241m=\u001b[39m DEFAULT_AGENT_RUNNER\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    201\u001b[0m     starting_agent,\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    203\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    204\u001b[0m     max_turns\u001b[38;5;241m=\u001b[39mmax_turns,\n\u001b[1;32m    205\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[1;32m    206\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    207\u001b[0m     previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[1;32m    208\u001b[0m )\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:397\u001b[0m, in \u001b[0;36mAgentRunner.run\u001b[0;34m(self, starting_agent, input, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    394\u001b[0m )\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_turn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     input_guardrail_results, turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_input_guardrails(\n\u001b[1;32m    399\u001b[0m             starting_agent,\n\u001b[1;32m    400\u001b[0m             starting_agent\u001b[38;5;241m.\u001b[39minput_guardrails\n\u001b[1;32m    401\u001b[0m             \u001b[38;5;241m+\u001b[39m (run_config\u001b[38;5;241m.\u001b[39minput_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[1;32m    402\u001b[0m             copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[1;32m    403\u001b[0m             context_wrapper,\n\u001b[1;32m    404\u001b[0m         ),\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[1;32m    406\u001b[0m             agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[1;32m    407\u001b[0m             all_tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[1;32m    408\u001b[0m             original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[1;32m    409\u001b[0m             generated_items\u001b[38;5;241m=\u001b[39mgenerated_items,\n\u001b[1;32m    410\u001b[0m             hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[1;32m    411\u001b[0m             context_wrapper\u001b[38;5;241m=\u001b[39mcontext_wrapper,\n\u001b[1;32m    412\u001b[0m             run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    413\u001b[0m             should_run_agent_start_hooks\u001b[38;5;241m=\u001b[39mshould_run_agent_start_hooks,\n\u001b[1;32m    414\u001b[0m             tool_use_tracker\u001b[38;5;241m=\u001b[39mtool_use_tracker,\n\u001b[1;32m    415\u001b[0m             previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[1;32m    416\u001b[0m         ),\n\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[1;32m    420\u001b[0m         agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[1;32m    421\u001b[0m         all_tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m         previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[1;32m    430\u001b[0m     )\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:997\u001b[0m, in \u001b[0;36mAgentRunner._run_input_guardrails\u001b[0;34m(cls, agent, guardrails, input, context)\u001b[0m\n\u001b[1;32m    994\u001b[0m guardrail_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m done \u001b[38;5;129;01min\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mas_completed(guardrail_tasks):\n\u001b[0;32m--> 997\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m done\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtripwire_triggered:\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# Cancel all guardrail tasks if a tripwire is triggered.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m guardrail_tasks:\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/asyncio/tasks.py:571\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/_run_impl.py:892\u001b[0m, in \u001b[0;36mRunImpl.run_single_input_guardrail\u001b[0;34m(cls, agent, guardrail, input, context)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single_input_guardrail\u001b[39m(\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    889\u001b[0m     context: RunContextWrapper[TContext],\n\u001b[1;32m    890\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InputGuardrailResult:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m guardrail_span(guardrail\u001b[38;5;241m.\u001b[39mget_name()) \u001b[38;5;28;01mas\u001b[39;00m span_guardrail:\n\u001b[0;32m--> 892\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m guardrail\u001b[38;5;241m.\u001b[39mrun(agent, \u001b[38;5;28minput\u001b[39m, context)\n\u001b[1;32m    893\u001b[0m         span_guardrail\u001b[38;5;241m.\u001b[39mspan_data\u001b[38;5;241m.\u001b[39mtriggered \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtripwire_triggered\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/guardrail.py:118\u001b[0m, in \u001b[0;36mInputGuardrail.run\u001b[0;34m(self, agent, input, context)\u001b[0m\n\u001b[1;32m    114\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguardrail_function(context, agent, \u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(output):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InputGuardrailResult(\n\u001b[1;32m    117\u001b[0m         guardrail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m--> 118\u001b[0m         output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m output,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m InputGuardrailResult(\n\u001b[1;32m    122\u001b[0m     guardrail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     output\u001b[38;5;241m=\u001b[39moutput,\n\u001b[1;32m    124\u001b[0m )\n",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m, in \u001b[0;36mhomework_guardrail\u001b[0;34m(ctx, agent, input_data)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhomework_guardrail\u001b[39m(ctx, agent, input_data):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# 传入护栏代理、输入数据、上下文\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(guardrail_agent, input_data, context\u001b[38;5;241m=\u001b[39mctx\u001b[38;5;241m.\u001b[39mcontext)\n\u001b[1;32m     28\u001b[0m     final_output \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mfinal_output_as(HomeworkOutput)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# 将结果转换为 HomeworkOutput 格式\u001b[39;00m\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:200\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03moutput is generated. The loop runs like so:\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m1. The agent is invoked with the given input.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    agent. Agents may perform handoffs, so we don't know the specific type of the output.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m runner \u001b[38;5;241m=\u001b[39m DEFAULT_AGENT_RUNNER\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    201\u001b[0m     starting_agent,\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    203\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    204\u001b[0m     max_turns\u001b[38;5;241m=\u001b[39mmax_turns,\n\u001b[1;32m    205\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[1;32m    206\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    207\u001b[0m     previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[1;32m    208\u001b[0m )\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:397\u001b[0m, in \u001b[0;36mAgentRunner.run\u001b[0;34m(self, starting_agent, input, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    394\u001b[0m )\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_turn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     input_guardrail_results, turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_input_guardrails(\n\u001b[1;32m    399\u001b[0m             starting_agent,\n\u001b[1;32m    400\u001b[0m             starting_agent\u001b[38;5;241m.\u001b[39minput_guardrails\n\u001b[1;32m    401\u001b[0m             \u001b[38;5;241m+\u001b[39m (run_config\u001b[38;5;241m.\u001b[39minput_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[1;32m    402\u001b[0m             copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[1;32m    403\u001b[0m             context_wrapper,\n\u001b[1;32m    404\u001b[0m         ),\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[1;32m    406\u001b[0m             agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[1;32m    407\u001b[0m             all_tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[1;32m    408\u001b[0m             original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[1;32m    409\u001b[0m             generated_items\u001b[38;5;241m=\u001b[39mgenerated_items,\n\u001b[1;32m    410\u001b[0m             hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[1;32m    411\u001b[0m             context_wrapper\u001b[38;5;241m=\u001b[39mcontext_wrapper,\n\u001b[1;32m    412\u001b[0m             run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    413\u001b[0m             should_run_agent_start_hooks\u001b[38;5;241m=\u001b[39mshould_run_agent_start_hooks,\n\u001b[1;32m    414\u001b[0m             tool_use_tracker\u001b[38;5;241m=\u001b[39mtool_use_tracker,\n\u001b[1;32m    415\u001b[0m             previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[1;32m    416\u001b[0m         ),\n\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[1;32m    420\u001b[0m         agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[1;32m    421\u001b[0m         all_tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m         previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[1;32m    430\u001b[0m     )\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:910\u001b[0m, in \u001b[0;36mAgentRunner._run_single_turn\u001b[0;34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m ItemHelpers\u001b[38;5;241m.\u001b[39minput_to_new_input_list(original_input)\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mextend([generated_item\u001b[38;5;241m.\u001b[39mto_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[0;32m--> 910\u001b[0m new_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_new_response(\n\u001b[1;32m    911\u001b[0m     agent,\n\u001b[1;32m    912\u001b[0m     system_prompt,\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    914\u001b[0m     output_schema,\n\u001b[1;32m    915\u001b[0m     all_tools,\n\u001b[1;32m    916\u001b[0m     handoffs,\n\u001b[1;32m    917\u001b[0m     context_wrapper,\n\u001b[1;32m    918\u001b[0m     run_config,\n\u001b[1;32m    919\u001b[0m     tool_use_tracker,\n\u001b[1;32m    920\u001b[0m     previous_response_id,\n\u001b[1;32m    921\u001b[0m     prompt_config,\n\u001b[1;32m    922\u001b[0m )\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_step_result_from_response(\n\u001b[1;32m    925\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[1;32m    926\u001b[0m     original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m     tool_use_tracker\u001b[38;5;241m=\u001b[39mtool_use_tracker,\n\u001b[1;32m    936\u001b[0m )\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:1071\u001b[0m, in \u001b[0;36mAgentRunner._get_new_response\u001b[0;34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id, prompt_config)\u001b[0m\n\u001b[1;32m   1068\u001b[0m model_settings \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mresolve(run_config\u001b[38;5;241m.\u001b[39mmodel_settings)\n\u001b[1;32m   1069\u001b[0m model_settings \u001b[38;5;241m=\u001b[39m RunImpl\u001b[38;5;241m.\u001b[39mmaybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[0;32m-> 1071\u001b[0m new_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[1;32m   1072\u001b[0m     system_instructions\u001b[38;5;241m=\u001b[39msystem_prompt,\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1074\u001b[0m     model_settings\u001b[38;5;241m=\u001b[39mmodel_settings,\n\u001b[1;32m   1075\u001b[0m     tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[1;32m   1076\u001b[0m     output_schema\u001b[38;5;241m=\u001b[39moutput_schema,\n\u001b[1;32m   1077\u001b[0m     handoffs\u001b[38;5;241m=\u001b[39mhandoffs,\n\u001b[1;32m   1078\u001b[0m     tracing\u001b[38;5;241m=\u001b[39mget_model_tracing_impl(\n\u001b[1;32m   1079\u001b[0m         run_config\u001b[38;5;241m.\u001b[39mtracing_disabled, run_config\u001b[38;5;241m.\u001b[39mtrace_include_sensitive_data\n\u001b[1;32m   1080\u001b[0m     ),\n\u001b[1;32m   1081\u001b[0m     previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[1;32m   1082\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt_config,\n\u001b[1;32m   1083\u001b[0m )\n\u001b[1;32m   1085\u001b[0m context_wrapper\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39madd(new_response\u001b[38;5;241m.\u001b[39musage)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/extensions/models/litellm_model.py:82\u001b[0m, in \u001b[0;36mLitellmModel.get_response\u001b[0;34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, prompt)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_response\u001b[39m(\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     66\u001b[0m     system_instructions: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     prompt: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelResponse:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[1;32m     77\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel),\n\u001b[1;32m     78\u001b[0m         model_config\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mto_json_dict()\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_impl\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     80\u001b[0m         disabled\u001b[38;5;241m=\u001b[39mtracing\u001b[38;5;241m.\u001b[39mis_disabled(),\n\u001b[1;32m     81\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[0;32m---> 82\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_response(\n\u001b[1;32m     83\u001b[0m             system_instructions,\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     85\u001b[0m             model_settings,\n\u001b[1;32m     86\u001b[0m             tools,\n\u001b[1;32m     87\u001b[0m             output_schema,\n\u001b[1;32m     88\u001b[0m             handoffs,\n\u001b[1;32m     89\u001b[0m             span_generation,\n\u001b[1;32m     90\u001b[0m             tracing,\n\u001b[1;32m     91\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m], litellm\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mChoices)\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _debug\u001b[38;5;241m.\u001b[39mDONT_LOG_MODEL_DATA:\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/extensions/models/litellm_model.py:301\u001b[0m, in \u001b[0;36mLitellmModel._fetch_response\u001b[0;34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream, prompt)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_settings\u001b[38;5;241m.\u001b[39mextra_args:\n\u001b[1;32m    299\u001b[0m     extra_kwargs\u001b[38;5;241m.\u001b[39mupdate(model_settings\u001b[38;5;241m.\u001b[39mextra_args)\n\u001b[0;32m--> 301\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39macompletion(\n\u001b[1;32m    302\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    303\u001b[0m     messages\u001b[38;5;241m=\u001b[39mconverted_messages,\n\u001b[1;32m    304\u001b[0m     tools\u001b[38;5;241m=\u001b[39mconverted_tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    305\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mtemperature,\n\u001b[1;32m    306\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mtop_p,\n\u001b[1;32m    307\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mfrequency_penalty,\n\u001b[1;32m    308\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mpresence_penalty,\n\u001b[1;32m    309\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mmax_tokens,\n\u001b[1;32m    310\u001b[0m     tool_choice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_not_given(tool_choice),\n\u001b[1;32m    311\u001b[0m     response_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_not_given(response_format),\n\u001b[1;32m    312\u001b[0m     parallel_tool_calls\u001b[38;5;241m=\u001b[39mparallel_tool_calls,\n\u001b[1;32m    313\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    314\u001b[0m     stream_options\u001b[38;5;241m=\u001b[39mstream_options,\n\u001b[1;32m    315\u001b[0m     reasoning_effort\u001b[38;5;241m=\u001b[39mreasoning_effort,\n\u001b[1;32m    316\u001b[0m     extra_headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mHEADERS, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(model_settings\u001b[38;5;241m.\u001b[39mextra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[1;32m    317\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key,\n\u001b[1;32m    318\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url,\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kwargs,\n\u001b[1;32m    320\u001b[0m )\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, litellm\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mModelResponse):\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/utils.py:1552\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper_async\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m timeout \u001b[38;5;241m=\u001b[39m _get_wrapper_timeout(kwargs\u001b[38;5;241m=\u001b[39mkwargs, exception\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout)\n\u001b[0;32m-> 1552\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/utils.py:1410\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper_async\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1407\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1410\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1411\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[1;32m   1413\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   1414\u001b[0m     call_type\u001b[38;5;241m=\u001b[39mcall_type,\n\u001b[1;32m   1415\u001b[0m ):\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/main.py:560\u001b[0m, in \u001b[0;36macompletion\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    559\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 560\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2293\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2292\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m/devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:482\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m422\u001b[39m:\n\u001b[1;32m    481\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(\n\u001b[1;32m    483\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    484\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    485\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    486\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    487\u001b[0m         litellm_debug_info\u001b[38;5;241m=\u001b[39mextra_information,\n\u001b[1;32m    488\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    489\u001b[0m     )\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[1;32m    491\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: DeepseekException - Failed to deserialize the JSON body into the target type: response_format: This response_format type is unavailable now at line 1 column 813"
     ]
    }
   ],
   "source": [
    "from agents import Runner\n",
    "from agents.exceptions import InputGuardrailTripwireTriggered\n",
    "\n",
    "# 测试数学问题（应该通过护栏）\n",
    "try:\n",
    "    result = await Runner.run(triage_agent, \"欧拉公式的应用？\")\n",
    "    print(\"结果:\", result.final_output)\n",
    "except InputGuardrailTripwireTriggered as e:\n",
    "    print(\"护栏触发：你的问题不是作业相关\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 测试历史问题（应该通过护栏）\n",
    "try:\n",
    "    result = await Runner.run(triage_agent, \"美国第一任总统是谁？\")\n",
    "    print(\"结果:\", result.final_output)\n",
    "except InputGuardrailTripwireTriggered as e:\n",
    "    print(\"护栏触发：你的问题不是作业相关\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 测试非学术问题（应该触发护栏）\n",
    "try:\n",
    "    result = await Runner.run(triage_agent, \"今天天气怎么样？\")\n",
    "    print(\"结果:\", result.final_output)\n",
    "except InputGuardrailTripwireTriggered as e:\n",
    "    print(\"护栏触发：你的问题不是作业相关\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner\n",
    "import asyncio\n",
    "from agents.exceptions import InputGuardrailTripwireTriggered\n",
    "\n",
    "# async def main():\n",
    "#     result = await Runner.run(triage_agent, \"does drinking tee good for the body?\")\n",
    "#     print(result.final_output)\n",
    "\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "    # asyncio.run(main())\n",
    "\n",
    "result = await Runner.run(triage_agent, \"欧拉公式的应用？\")\n",
    "print(result.final_output)\n",
    "\n",
    "\n",
    "# try:\n",
    "#     result = await Runner.run(triage_agent, \"who was the first president of the united states?\")\n",
    "#     print(result.final_output)\n",
    "# except InputGuardrailTripwireTriggered as e:\n",
    "#     print(\"你的问题不是作业相关\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "报错没有解决！\n",
    "\n",
    "```shell\n",
    "---------------------------------------------------------------------------\n",
    "HTTPStatusError                           Traceback (most recent call last)\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:111, in BaseLLMHTTPHandler._make_common_async_call(self, async_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\n",
    "    110 try:\n",
    "--> 111     response = await async_httpx_client.post(\n",
    "    112         url=api_base,\n",
    "    113         headers=headers,\n",
    "    114         data=(\n",
    "    115             signed_json_body\n",
    "    116             if signed_json_body is not None\n",
    "    117             else json.dumps(data)\n",
    "    118         ),\n",
    "    119         timeout=timeout,\n",
    "    120         stream=stream,\n",
    "    121         logging_obj=logging_obj,\n",
    "    122     )\n",
    "    123 except httpx.HTTPStatusError as e:\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/litellm_core_utils/logging_utils.py:135, in track_llm_api_timing.<locals>.decorator.<locals>.async_wrapper(*args, **kwargs)\n",
    "    134 try:\n",
    "--> 135     result = await func(*args, **kwargs)\n",
    "    136     return result\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py:324, in AsyncHTTPHandler.post(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\n",
    "    322     setattr(e, \"status_code\", e.response.status_code)\n",
    "--> 324     raise e\n",
    "    325 except Exception as e:\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py:280, in AsyncHTTPHandler.post(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\n",
    "    279 response = await self.client.send(req, stream=stream)\n",
    "--> 280 response.raise_for_status()\n",
    "    281 return response\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/httpx/_models.py:829, in Response.raise_for_status(self)\n",
    "    828 message = message.format(self, error_type=error_type)\n",
    "--> 829 raise HTTPStatusError(message, request=request, response=self)\n",
    "\n",
    "HTTPStatusError: Client error '422 Unprocessable Entity' for url 'https://api.deepseek.com/v1/chat/completions'\n",
    "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "OpenAIError                               Traceback (most recent call last)\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/main.py:541, in acompletion(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, **kwargs)\n",
    "    540 elif asyncio.iscoroutine(init_response):\n",
    "--> 541     response = await init_response\n",
    "    542 else:\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:238, in BaseLLMHTTPHandler.async_completion(self, custom_llm_provider, provider_config, api_base, headers, data, timeout, model, model_response, logging_obj, messages, optional_params, litellm_params, encoding, api_key, client, json_mode, signed_json_body)\n",
    "    236     async_httpx_client = client\n",
    "--> 238 response = await self._make_common_async_call(\n",
    "    239     async_httpx_client=async_httpx_client,\n",
    "    240     provider_config=provider_config,\n",
    "    241     api_base=api_base,\n",
    "    242     headers=headers,\n",
    "    243     data=data,\n",
    "    244     timeout=timeout,\n",
    "    245     litellm_params=litellm_params,\n",
    "    246     stream=False,\n",
    "    247     logging_obj=logging_obj,\n",
    "    248     signed_json_body=signed_json_body,\n",
    "    249 )\n",
    "    250 return provider_config.transform_response(\n",
    "    251     model=model,\n",
    "    252     raw_response=response,\n",
    "   (...)\n",
    "    261     json_mode=json_mode,\n",
    "    262 )\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:136, in BaseLLMHTTPHandler._make_common_async_call(self, async_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\n",
    "    135     else:\n",
    "--> 136         raise self._handle_error(e=e, provider_config=provider_config)\n",
    "    137 except Exception as e:\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:2405, in BaseLLMHTTPHandler._handle_error(self, e, provider_config)\n",
    "   2399     raise BaseLLMException(\n",
    "   2400         status_code=status_code,\n",
    "   2401         message=error_text,\n",
    "   2402         headers=error_headers,\n",
    "   2403     )\n",
    "-> 2405 raise provider_config.get_error_class(\n",
    "   2406     error_message=error_text,\n",
    "   2407     status_code=status_code,\n",
    "   2408     headers=error_headers,\n",
    "   2409 )\n",
    "\n",
    "OpenAIError: Failed to deserialize the JSON body into the target type: response_format: This response_format type is unavailable now at line 1 column 633\n",
    "\n",
    "During handling of the above exception, another exception occurred:\n",
    "\n",
    "BadRequestError                           Traceback (most recent call last)\n",
    "Cell In[5], line 18\n",
    "      6     print(result.final_output)\n",
    "      9 # async def main():\n",
    "     10 #     result = await Runner.run(triage_agent, \"does drinking tee good for the body?\")\n",
    "     11 #     print(result.final_output)\n",
    "   (...)\n",
    "     14 # if __name__ == \"__main__\":\n",
    "     15     # asyncio.run(main())\n",
    "---> 18 result = await Runner.run(triage_agent, \"does drinking tee good for the body?\")\n",
    "     19 print(result.final_output)\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:200, in Runner.run(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\n",
    "    173 \"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\n",
    "    174 output is generated. The loop runs like so:\n",
    "    175 1. The agent is invoked with the given input.\n",
    "   (...)\n",
    "    197     agent. Agents may perform handoffs, so we don't know the specific type of the output.\n",
    "    198 \"\"\"\n",
    "    199 runner = DEFAULT_AGENT_RUNNER\n",
    "--> 200 return await runner.run(\n",
    "    201     starting_agent,\n",
    "    202     input,\n",
    "    203     context=context,\n",
    "    204     max_turns=max_turns,\n",
    "    205     hooks=hooks,\n",
    "    206     run_config=run_config,\n",
    "    207     previous_response_id=previous_response_id,\n",
    "    208 )\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:397, in AgentRunner.run(self, starting_agent, input, **kwargs)\n",
    "    392 logger.debug(\n",
    "    393     f\"Running agent {current_agent.name} (turn {current_turn})\",\n",
    "    394 )\n",
    "    396 if current_turn == 1:\n",
    "--> 397     input_guardrail_results, turn_result = await asyncio.gather(\n",
    "    398         self._run_input_guardrails(\n",
    "    399             starting_agent,\n",
    "    400             starting_agent.input_guardrails\n",
    "    401             + (run_config.input_guardrails or []),\n",
    "    402             copy.deepcopy(input),\n",
    "    403             context_wrapper,\n",
    "    404         ),\n",
    "    405         self._run_single_turn(\n",
    "    406             agent=current_agent,\n",
    "    407             all_tools=all_tools,\n",
    "    408             original_input=original_input,\n",
    "    409             generated_items=generated_items,\n",
    "    410             hooks=hooks,\n",
    "    411             context_wrapper=context_wrapper,\n",
    "    412             run_config=run_config,\n",
    "    413             should_run_agent_start_hooks=should_run_agent_start_hooks,\n",
    "    414             tool_use_tracker=tool_use_tracker,\n",
    "    415             previous_response_id=previous_response_id,\n",
    "    416         ),\n",
    "    417     )\n",
    "    418 else:\n",
    "    419     turn_result = await self._run_single_turn(\n",
    "    420         agent=current_agent,\n",
    "    421         all_tools=all_tools,\n",
    "   (...)\n",
    "    429         previous_response_id=previous_response_id,\n",
    "    430     )\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:997, in AgentRunner._run_input_guardrails(cls, agent, guardrails, input, context)\n",
    "    994 guardrail_results = []\n",
    "    996 for done in asyncio.as_completed(guardrail_tasks):\n",
    "--> 997     result = await done\n",
    "    998     if result.output.tripwire_triggered:\n",
    "    999         # Cancel all guardrail tasks if a tripwire is triggered.\n",
    "   1000         for t in guardrail_tasks:\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/asyncio/tasks.py:571, in as_completed.<locals>._wait_for_one()\n",
    "    568 if f is None:\n",
    "    569     # Dummy value from _on_timeout().\n",
    "    570     raise exceptions.TimeoutError\n",
    "--> 571 return f.result()\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/_run_impl.py:892, in RunImpl.run_single_input_guardrail(cls, agent, guardrail, input, context)\n",
    "    883 @classmethod\n",
    "    884 async def run_single_input_guardrail(\n",
    "    885     cls,\n",
    "   (...)\n",
    "    889     context: RunContextWrapper[TContext],\n",
    "    890 ) -> InputGuardrailResult:\n",
    "    891     with guardrail_span(guardrail.get_name()) as span_guardrail:\n",
    "--> 892         result = await guardrail.run(agent, input, context)\n",
    "    893         span_guardrail.span_data.triggered = result.output.tripwire_triggered\n",
    "    894         return result\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/guardrail.py:118, in InputGuardrail.run(self, agent, input, context)\n",
    "    114 output = self.guardrail_function(context, agent, input)\n",
    "    115 if inspect.isawaitable(output):\n",
    "    116     return InputGuardrailResult(\n",
    "    117         guardrail=self,\n",
    "--> 118         output=await output,\n",
    "    119     )\n",
    "    121 return InputGuardrailResult(\n",
    "    122     guardrail=self,\n",
    "    123     output=output,\n",
    "    124 )\n",
    "\n",
    "Cell In[3], line 20\n",
    "     18 async def homework_guardrail(ctx, agent, input_data):\n",
    "     19     # 传入护栏代理、输入数据、上下文\n",
    "---> 20     result = await Runner.run(guardrail_agent, input_data, context=ctx.context)\n",
    "     21     final_output = result.final_output_as(HomeworkOutput)\n",
    "     22     # 将结果转换为 HomeworkOutput 格式\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:200, in Runner.run(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\n",
    "    173 \"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\n",
    "    174 output is generated. The loop runs like so:\n",
    "    175 1. The agent is invoked with the given input.\n",
    "   (...)\n",
    "    197     agent. Agents may perform handoffs, so we don't know the specific type of the output.\n",
    "    198 \"\"\"\n",
    "    199 runner = DEFAULT_AGENT_RUNNER\n",
    "--> 200 return await runner.run(\n",
    "    201     starting_agent,\n",
    "    202     input,\n",
    "    203     context=context,\n",
    "    204     max_turns=max_turns,\n",
    "    205     hooks=hooks,\n",
    "    206     run_config=run_config,\n",
    "    207     previous_response_id=previous_response_id,\n",
    "    208 )\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:397, in AgentRunner.run(self, starting_agent, input, **kwargs)\n",
    "    392 logger.debug(\n",
    "    393     f\"Running agent {current_agent.name} (turn {current_turn})\",\n",
    "    394 )\n",
    "    396 if current_turn == 1:\n",
    "--> 397     input_guardrail_results, turn_result = await asyncio.gather(\n",
    "    398         self._run_input_guardrails(\n",
    "    399             starting_agent,\n",
    "    400             starting_agent.input_guardrails\n",
    "    401             + (run_config.input_guardrails or []),\n",
    "    402             copy.deepcopy(input),\n",
    "    403             context_wrapper,\n",
    "    404         ),\n",
    "    405         self._run_single_turn(\n",
    "    406             agent=current_agent,\n",
    "    407             all_tools=all_tools,\n",
    "    408             original_input=original_input,\n",
    "    409             generated_items=generated_items,\n",
    "    410             hooks=hooks,\n",
    "    411             context_wrapper=context_wrapper,\n",
    "    412             run_config=run_config,\n",
    "    413             should_run_agent_start_hooks=should_run_agent_start_hooks,\n",
    "    414             tool_use_tracker=tool_use_tracker,\n",
    "    415             previous_response_id=previous_response_id,\n",
    "    416         ),\n",
    "    417     )\n",
    "    418 else:\n",
    "    419     turn_result = await self._run_single_turn(\n",
    "    420         agent=current_agent,\n",
    "    421         all_tools=all_tools,\n",
    "   (...)\n",
    "    429         previous_response_id=previous_response_id,\n",
    "    430     )\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:910, in AgentRunner._run_single_turn(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\n",
    "    907 input = ItemHelpers.input_to_new_input_list(original_input)\n",
    "    908 input.extend([generated_item.to_input_item() for generated_item in generated_items])\n",
    "--> 910 new_response = await cls._get_new_response(\n",
    "    911     agent,\n",
    "    912     system_prompt,\n",
    "    913     input,\n",
    "    914     output_schema,\n",
    "    915     all_tools,\n",
    "    916     handoffs,\n",
    "    917     context_wrapper,\n",
    "    918     run_config,\n",
    "    919     tool_use_tracker,\n",
    "    920     previous_response_id,\n",
    "    921     prompt_config,\n",
    "    922 )\n",
    "    924 return await cls._get_single_step_result_from_response(\n",
    "    925     agent=agent,\n",
    "    926     original_input=original_input,\n",
    "   (...)\n",
    "    935     tool_use_tracker=tool_use_tracker,\n",
    "    936 )\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/run.py:1071, in AgentRunner._get_new_response(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id, prompt_config)\n",
    "   1068 model_settings = agent.model_settings.resolve(run_config.model_settings)\n",
    "   1069 model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n",
    "-> 1071 new_response = await model.get_response(\n",
    "   1072     system_instructions=system_prompt,\n",
    "   1073     input=input,\n",
    "   1074     model_settings=model_settings,\n",
    "   1075     tools=all_tools,\n",
    "   1076     output_schema=output_schema,\n",
    "   1077     handoffs=handoffs,\n",
    "   1078     tracing=get_model_tracing_impl(\n",
    "   1079         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n",
    "   1080     ),\n",
    "   1081     previous_response_id=previous_response_id,\n",
    "   1082     prompt=prompt_config,\n",
    "   1083 )\n",
    "   1085 context_wrapper.usage.add(new_response.usage)\n",
    "   1087 return new_response\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/extensions/models/litellm_model.py:82, in LitellmModel.get_response(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, prompt)\n",
    "     64 async def get_response(\n",
    "     65     self,\n",
    "     66     system_instructions: str | None,\n",
    "   (...)\n",
    "     74     prompt: Any | None = None,\n",
    "     75 ) -> ModelResponse:\n",
    "     76     with generation_span(\n",
    "     77         model=str(self.model),\n",
    "     78         model_config=model_settings.to_json_dict()\n",
    "     79         | {\"base_url\": str(self.base_url or \"\"), \"model_impl\": \"litellm\"},\n",
    "     80         disabled=tracing.is_disabled(),\n",
    "     81     ) as span_generation:\n",
    "---> 82         response = await self._fetch_response(\n",
    "     83             system_instructions,\n",
    "     84             input,\n",
    "     85             model_settings,\n",
    "     86             tools,\n",
    "     87             output_schema,\n",
    "     88             handoffs,\n",
    "     89             span_generation,\n",
    "     90             tracing,\n",
    "     91             stream=False,\n",
    "     92             prompt=prompt,\n",
    "     93         )\n",
    "     95         assert isinstance(response.choices[0], litellm.types.utils.Choices)\n",
    "     97         if _debug.DONT_LOG_MODEL_DATA:\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/agents/extensions/models/litellm_model.py:301, in LitellmModel._fetch_response(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream, prompt)\n",
    "    298 if model_settings.extra_args:\n",
    "    299     extra_kwargs.update(model_settings.extra_args)\n",
    "--> 301 ret = await litellm.acompletion(\n",
    "    302     model=self.model,\n",
    "    303     messages=converted_messages,\n",
    "    304     tools=converted_tools or None,\n",
    "    305     temperature=model_settings.temperature,\n",
    "    306     top_p=model_settings.top_p,\n",
    "    307     frequency_penalty=model_settings.frequency_penalty,\n",
    "    308     presence_penalty=model_settings.presence_penalty,\n",
    "    309     max_tokens=model_settings.max_tokens,\n",
    "    310     tool_choice=self._remove_not_given(tool_choice),\n",
    "    311     response_format=self._remove_not_given(response_format),\n",
    "    312     parallel_tool_calls=parallel_tool_calls,\n",
    "    313     stream=stream,\n",
    "    314     stream_options=stream_options,\n",
    "    315     reasoning_effort=reasoning_effort,\n",
    "    316     extra_headers={**HEADERS, **(model_settings.extra_headers or {})},\n",
    "    317     api_key=self.api_key,\n",
    "    318     base_url=self.base_url,\n",
    "    319     **extra_kwargs,\n",
    "    320 )\n",
    "    322 if isinstance(ret, litellm.types.utils.ModelResponse):\n",
    "    323     return ret\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/utils.py:1552, in client.<locals>.wrapper_async(*args, **kwargs)\n",
    "   1550 timeout = _get_wrapper_timeout(kwargs=kwargs, exception=e)\n",
    "   1551 setattr(e, \"timeout\", timeout)\n",
    "-> 1552 raise e\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/utils.py:1410, in client.<locals>.wrapper_async(*args, **kwargs)\n",
    "   1407         print_verbose(f\"Error while checking max token limit: {str(e)}\")\n",
    "   1409 # MODEL CALL\n",
    "-> 1410 result = await original_function(*args, **kwargs)\n",
    "   1411 end_time = datetime.datetime.now()\n",
    "   1412 if _is_streaming_request(\n",
    "   1413     kwargs=kwargs,\n",
    "   1414     call_type=call_type,\n",
    "   1415 ):\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/main.py:560, in acompletion(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, **kwargs)\n",
    "    558 except Exception as e:\n",
    "    559     custom_llm_provider = custom_llm_provider or \"openai\"\n",
    "--> 560     raise exception_type(\n",
    "    561         model=model,\n",
    "    562         custom_llm_provider=custom_llm_provider,\n",
    "    563         original_exception=e,\n",
    "    564         completion_kwargs=completion_kwargs,\n",
    "    565         extra_kwargs=kwargs,\n",
    "    566     )\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2293, in exception_type(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\n",
    "   2291 if exception_mapping_worked:\n",
    "   2292     setattr(e, \"litellm_response_headers\", litellm_response_headers)\n",
    "-> 2293     raise e\n",
    "   2294 else:\n",
    "   2295     for error_type in litellm.LITELLM_EXCEPTION_TYPES:\n",
    "\n",
    "File /devtool/anaconda3/envs/openai-agent/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:482, in exception_type(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\n",
    "    480 elif original_exception.status_code == 422:\n",
    "    481     exception_mapping_worked = True\n",
    "--> 482     raise BadRequestError(\n",
    "    483         message=f\"{exception_provider} - {message}\",\n",
    "    484         model=model,\n",
    "    485         llm_provider=custom_llm_provider,\n",
    "    486         response=getattr(original_exception, \"response\", None),\n",
    "    487         litellm_debug_info=extra_information,\n",
    "    488         body=getattr(original_exception, \"body\", None),\n",
    "    489     )\n",
    "    490 elif original_exception.status_code == 429:\n",
    "    491     exception_mapping_worked = True\n",
    "\n",
    "BadRequestError: litellm.BadRequestError: DeepseekException - Failed to deserialize the JSON body into the target type: response_format: This response_format type is unavailable now at line 1 column 633```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
