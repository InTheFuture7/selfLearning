{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几种输出方式：流式输出、同时回答多个 instruction、考虑上一轮对话、交互式输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 流式输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同于将一整段回答直接输出，流式输出是将模型输出过程中的每一小步/部分内容作为一个个事件，每个事件可能表示几个字或图片或其他，通过控制事件达到流式输出\n",
    "\n",
    "run_streamed() 不能使用 await"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "# 从环境变量中读取api_key\n",
    "api_key = os.getenv('mistral_key')\n",
    "base_url = 'https://api.mistral.ai/v1'\n",
    "chat_model = \"mistral/mistral-small-latest\"\n",
    "set_tracing_disabled(disabled=True)\n",
    "llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"天气助手\",\n",
    "    instructions=\"始终用汉赋的形式回答用户\",\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    result = Runner.run_streamed(agent, \"给我讲个程序员相亲的笑话\")\n",
    "    async for event in result.stream_events():\n",
    "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            # 只关心指定类型的事件\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 列表输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "messages 可以理解为将多个 instructions 拼接到一个列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from agents import Agent, Runner, set_tracing_disabled\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "# 从环境变量中读取api_key\n",
    "api_key = os.getenv('mistral_key')\n",
    "base_url = 'https://api.mistral.ai/v1'\n",
    "chat_model = \"mistral/mistral-small-latest\"\n",
    "set_tracing_disabled(disabled=True)\n",
    "llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"小助手\",\n",
    "    instructions=\"回答用户的问题。\",\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"孔子的全名叫什么?\"},\n",
    "    {\"role\": \"user\", \"content\": \"孟子的全名叫什么?\"},\n",
    "    ]\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(\n",
    "        agent,\n",
    "        input=messages,\n",
    "    )\n",
    "    print(result.final_output)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 连续多轮对话"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将**之前的** instruction 及回答合并到下一个 instruction 中，实现考虑上下文的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from agents import Agent, Runner, set_tracing_disabled\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "# 从环境变量中读取api_key\n",
    "api_key = os.getenv('mistral_key')\n",
    "base_url = 'https://api.mistral.ai/v1'\n",
    "chat_model = \"mistral/mistral-small-latest\"\n",
    "set_tracing_disabled(disabled=True)\n",
    "llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)\n",
    "\n",
    "agent = Agent(name=\"Assistant\", model=llm, instructions=\"Reply very concisely.\")\n",
    "\n",
    "async def main():\n",
    "    # First turn\n",
    "    result = await Runner.run(agent, \"What city is the Golden Gate Bridge in?\")\n",
    "    print(result.final_output)\n",
    "    # San Francisco\n",
    "    print(\"firt result.to_input_list()\", result.to_input_list())\n",
    "\n",
    "    # Second turn\n",
    "    new_input = result.to_input_list() + [{\"role\": \"user\", \"content\": \"What state is it in?\"}]\n",
    "    result = await Runner.run(agent, new_input)\n",
    "    print(result.final_output)\n",
    "\n",
    "    print(\"second result.to_input_list()\", result.to_input_list())\n",
    "\n",
    "    # Third\n",
    "    new_input_ = result.to_input_list() + [{\"role\": \"user\", \"content\": \"which bridge have we mentioned?\"}]\n",
    "    result = await Runner.run(agent, new_input_)\n",
    "    print(result.final_output)\n",
    "\n",
    "    print(\"third result.to_input_list()\", result.to_input_list())\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 终端的人类交互\n",
    "\n",
    "run_demo_loop 在循环中提示用户输入，保留回合之间的对话历史，可以实现连续多轮对话同样的效果\n",
    "\n",
    "默认情况下，它在生成模型输出时进行流式传输"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "from agents import Agent, Runner, set_tracing_disabled, run_demo_loop\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "# 从环境变量中读取api_key\n",
    "api_key = os.getenv('mistral_key')\n",
    "base_url = 'https://api.mistral.ai/v1'\n",
    "chat_model = \"mistral/mistral-small-latest\"\n",
    "set_tracing_disabled(disabled=True)\n",
    "llm = LitellmModel(model=chat_model, api_key=api_key, base_url=base_url)\n",
    "agent = Agent(name=\"Assistant\", model=llm, instructions=\"Reply very concisely.\")\n",
    "\n",
    "async def main():\n",
    "    await run_demo_loop(agent)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
